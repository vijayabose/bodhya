# Bodhya Model Manifest
# This file defines available models, their roles, and download sources

models:
  # Code domain models
  code_planner:
    role: planner
    domain: code
    display_name: "Code Planner (Qwen 2.5 Coder)"
    description: "Specialized model for task decomposition and BDD planning"
    source_url: "https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_k_m.gguf"
    size_gb: 4.4
    quantization: "Q4_K_M"
    checksum: "sha256:placeholder_checksum_for_planner"
    backend: local

  code_coder:
    role: coder
    domain: code
    display_name: "Code Generator (DeepSeek Coder)"
    description: "High-quality code generation model"
    source_url: "https://huggingface.co/TheBloke/deepseek-coder-6.7B-instruct-GGUF/resolve/main/deepseek-coder-6.7b-instruct.Q4_K_M.gguf"
    size_gb: 3.8
    quantization: "Q4_K_M"
    checksum: "sha256:placeholder_checksum_for_coder"
    backend: local

  code_reviewer:
    role: reviewer
    domain: code
    display_name: "Code Reviewer (CodeLlama)"
    description: "Model specialized in code review and critique"
    source_url: "https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-GGUF/resolve/main/codellama-7b-instruct.Q4_K_M.gguf"
    size_gb: 4.1
    quantization: "Q4_K_M"
    checksum: "sha256:placeholder_checksum_for_reviewer"
    backend: local

  # Mail domain models
  mail_writer:
    role: writer
    domain: mail
    display_name: "Email Writer (Mistral 7B)"
    description: "Professional email and communication writing"
    source_url: "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf"
    size_gb: 4.4
    quantization: "Q4_K_M"
    checksum: "sha256:placeholder_checksum_for_writer"
    backend: local

  # General purpose models
  general_small:
    role: general
    domain: general
    display_name: "General Purpose (Phi-3 Mini)"
    description: "Small, fast model for general tasks"
    source_url: "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf"
    size_gb: 2.3
    quantization: "Q4_0"
    checksum: "sha256:placeholder_checksum_for_general"
    backend: local

# Model backend configuration
backends:
  local:
    type: mistral_rs
    config:
      device: auto  # auto, cpu, cuda, metal
      num_threads: 4
      context_size: 4096

  remote:
    type: openai_compatible
    enabled: false
    config:
      api_base: "http://localhost:8000/v1"
      timeout_seconds: 30
